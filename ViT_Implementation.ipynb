{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Simple ViT using Pytorch [20 marks]\n",
    "\n",
    "In this question, you will implement a vision transformer based image classification model using pytorch. \n",
    "Implement a basic version of vision transformer (https://arxiv.org/pdf/2010.11929.pdf), that first divides an image into patches and then passes them through a set of multihead self attention modules to perform classification. Please check out the details in the paper. Note that classification happens from the CLS token of the final transformer layer.\n",
    "\n",
    "[Experiment 1] Train this model on the CIFAR-10 dataset for 10-class classification. Keep the number of attention heads to be 4 for all the experiments. [6 points]\n",
    "\n",
    "[Experiment 2] Try out different patch sizes (like 4x4, 8x8, 16x16). You can divide the image into both overlapping and non-overlapping patches. [4 points]\n",
    "\n",
    "[Experiment 3] How does model performance change if you vary the number of attention heads? [4 points]\n",
    "\n",
    "[Experiment 4] Perform classification by using the CLS token from different layers of the model. [6 points]\n",
    "Create a detailed report of all the experiments and analyses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Convert image into patches\n",
    "2. Vectorize the patches d1 X d2 X d3 --> d1d2d3 X 1 (One vector per patch)\n",
    "3. Dense layer to these vectors (no non-linear activation) All have same W and same b ; z1=w1Tx1 + b and postions ; x1 is say vector 1 (patch 1).Dense layer takes input of  postions to create z1,z2,..zn postional embeddings\n",
    "4. CLS token input to an embeddinglayer to create z0 vector (same shape as other z's)..o/pof transformer here used for classification\n",
    "5. z0,...zn are i/p to multiheaded self attention (n+1 vectors o/p)\n",
    "6. Apply a dense layer ((n+1 vectors o/p))\n",
    "7. add as many as Multi-headed self attention plus dense layers as u want ( jointly called transformer encoder network)\n",
    "8. At the last layer we focus on c0 vector and feed to a softmax classifier. o/p(say p) has shape equal to number of classes\n",
    "9. During training Loss is CE of p and GT\n",
    "10. Compute gradient descent. \n",
    "\n",
    "Potential Hyperparamters\n",
    "1. patch-sizes\n",
    "2. overlapping and non-overlapping counter\n",
    "3. number of attention heads\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pyplot\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "from torch.utils.data import DataLoader as dataloader\n",
    "import torchvision.datasets as datasets\n",
    "import torch.optim as optim\n",
    "import gc\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "train_data=datasets.CIFAR10(root=\"/root/DLVA Assig-2/Assignment2/Assignment2/dataset\", train=True, download=True,transform=transform)\n",
    "test_data=datasets.CIFAR10(root=\"/root/DLVA Assig-2/Assignment2/Assignment2/dataset\", train=False, download=True,transform=transform)\n",
    "\n",
    "train_dataloader=dataloader(train_data, batch_size=300, shuffle=True)\n",
    "test_dataloader=dataloader(test_data, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperprameters\n",
    "epochs=25\n",
    "patch_size=4\n",
    "overlapping=0 #indicates how many layers of overlapping between patches\n",
    "num_heads=4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akshe\\AppData\\Local\\Temp\\ipykernel_30512\\2722164733.py:79: DeprecationWarning: the imp module is deprecated in favour of importlib and slated for removal in Python 3.12; see the module's documentation for alternative uses\n",
      "  import imp\n"
     ]
    }
   ],
   "source": [
    "class patch(nn.Module):\n",
    "    def __init__(self,overlapping,patch_size,in_channels:int=3):\n",
    "        super(patch,self).__init__()        \n",
    "        self.patcher=nn.Conv2d(in_channels=in_channels,out_channels=patch_size*patch_size*3,kernel_size=patch_size,stride=patch_size-overlapping,padding=0)\n",
    "        self.Flatten=nn.Flatten(start_dim=2,end_dim=3)\n",
    "\n",
    "    def forward(self,x):\n",
    "    \n",
    "        x_patch=self.patcher(x)\n",
    "        x_flat=self.Flatten(x_patch)\n",
    "        return torch.permute(x_flat,(0,2,1))\n",
    "        \n",
    "class multi_head_attention(nn.Module):\n",
    "    def __init__(self,patch_size,num_heads):\n",
    "        super(multi_head_attention,self).__init__()\n",
    "\n",
    "        self.layer_norm=nn.LayerNorm(normalized_shape=patch_size*patch_size*3)\n",
    "        self.multihead_attn=nn.MultiheadAttention(embed_dim=patch_size*patch_size*3,num_heads=num_heads,batch_first=True)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layer_norm(x)\n",
    "        attn_ouput,_=self.multihead_attn(query=x,key=x,value=x,need_weights=False)\n",
    "        return attn_ouput\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self,patch_size,mlp_size:int=3072,dropout:float=0.1):\n",
    "        super(MLP,self).__init__()\n",
    "        self.layer_norm=nn.LayerNorm(normalized_shape=patch_size*patch_size*3)\n",
    "        self.mlp=nn.Sequential(\n",
    "            nn.Linear(patch_size*patch_size*3,mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout,inplace=True),\n",
    "            nn.Linear(mlp_size,patch_size*patch_size*3),\n",
    "            nn.Dropout(p=dropout,inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.layer_norm(x)\n",
    "        x=self.mlp(x)\n",
    "        return x\n",
    "        \n",
    "class Transformer_Encoder(nn.Module):\n",
    "    def __init__(self,patch_size=patch_size,num_heads=num_heads,mlp_size=3072,mlp_dropout:float=0.1):\n",
    "        super(Transformer_Encoder,self).__init__()\n",
    "        self.msa=multi_head_attention(patch_size,num_heads)\n",
    "        self.mlp=MLP(patch_size,mlp_size=mlp_size,dropout=mlp_dropout)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.msa(x) + x\n",
    "        x=self.mlp(x) + x\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViT(nn.Module):\n",
    "    def __init__(self,patch_size,num_heads,img_size:int=32,in_channels:int=3,num_t_layers=12,mlp_droput:float=0.1,\n",
    "                 mlp_size=3072,embedding_dropout:float=0.1,num_transformer_layers:int=10):\n",
    "        super(ViT,self).__init__()\n",
    "\n",
    "        self.num_patches= (img_size*img_size)//patch_size**2\n",
    "        self.class_embedding=nn.Parameter(data=torch.randn(1,1,patch_size*patch_size*3),requires_grad=True)\n",
    "        self.position_emdedding=nn.Parameter(data=torch.randn(1,self.num_patches+1,patch_size*patch_size*3),requires_grad=True)\n",
    "        self.embedding_drop=nn.Dropout(p=embedding_dropout)\n",
    "        self.patch_embedding=patch(in_channels=in_channels,overlapping=overlapping,patch_size=patch_size)\n",
    "        self.transformer_encoder=nn.Sequential(*[Transformer_Encoder(patch_size=patch_size,num_heads=num_heads,mlp_size=mlp_size,mlp_dropout=0.1) for _ in range(num_transformer_layers)])\n",
    "        self.classifier=nn.Sequential(nn.LayerNorm(normalized_shape=patch_size*patch_size*3),nn.Linear(in_features=patch_size*patch_size*3,out_features=10))\n",
    "\n",
    "    def forward(self,x):\n",
    "        batch_size=x.shape[0]\n",
    "        cls_token=self.class_embedding.expand(batch_size,-1,-1)\n",
    "        x=self.patch_embedding(x)\n",
    "        x=torch.cat((cls_token,x),dim=1)\n",
    "        x=self.position_emdedding+x\n",
    "        x=self.embedding_drop(x)\n",
    "        x=self.transformer_encoder(x)\n",
    "        x=self.classifier(x[:,0])\n",
    "      \n",
    "        return x\n",
    "                                                                            \n",
    "import imp\n",
    "\n",
    "file, pathname, description = imp.find_module('engine', ['./pytorch_deep_learning/going_modular/going_modular'])\n",
    "my_module = imp.load_module('engine', file, pathname, description)\n",
    "\n",
    "import engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e0ba17986e4cd9a87156ac5199795c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 2.0756 | train_acc: 0.2007 | test_loss: 1.8880 | test_acc: 0.2722\n",
      "Epoch: 2 | train_loss: 1.7820 | train_acc: 0.3341 | test_loss: 1.6573 | test_acc: 0.3883\n",
      "Epoch: 3 | train_loss: 1.6065 | train_acc: 0.4092 | test_loss: 1.5219 | test_acc: 0.4399\n",
      "Epoch: 4 | train_loss: 1.5299 | train_acc: 0.4386 | test_loss: 1.4715 | test_acc: 0.4576\n",
      "Epoch: 5 | train_loss: 1.4756 | train_acc: 0.4580 | test_loss: 1.4297 | test_acc: 0.4827\n",
      "Epoch: 6 | train_loss: 1.4272 | train_acc: 0.4767 | test_loss: 1.4056 | test_acc: 0.4924\n",
      "Epoch: 7 | train_loss: 1.3971 | train_acc: 0.4912 | test_loss: 1.3623 | test_acc: 0.5015\n",
      "Epoch: 8 | train_loss: 1.3630 | train_acc: 0.5028 | test_loss: 1.3274 | test_acc: 0.5164\n",
      "Epoch: 9 | train_loss: 1.3448 | train_acc: 0.5108 | test_loss: 1.3094 | test_acc: 0.5209\n",
      "Epoch: 10 | train_loss: 1.3184 | train_acc: 0.5211 | test_loss: 1.3084 | test_acc: 0.5239\n",
      "Epoch: 11 | train_loss: 1.3016 | train_acc: 0.5271 | test_loss: 1.3015 | test_acc: 0.5244\n",
      "Epoch: 12 | train_loss: 1.2890 | train_acc: 0.5320 | test_loss: 1.2913 | test_acc: 0.5342\n",
      "Epoch: 13 | train_loss: 1.2648 | train_acc: 0.5409 | test_loss: 1.2719 | test_acc: 0.5329\n",
      "Epoch: 14 | train_loss: 1.2470 | train_acc: 0.5487 | test_loss: 1.2614 | test_acc: 0.5410\n",
      "Epoch: 15 | train_loss: 1.2348 | train_acc: 0.5505 | test_loss: 1.2464 | test_acc: 0.5490\n",
      "Epoch: 16 | train_loss: 1.2192 | train_acc: 0.5567 | test_loss: 1.2249 | test_acc: 0.5541\n",
      "Epoch: 17 | train_loss: 1.2014 | train_acc: 0.5643 | test_loss: 1.2152 | test_acc: 0.5547\n",
      "Epoch: 18 | train_loss: 1.1892 | train_acc: 0.5689 | test_loss: 1.2412 | test_acc: 0.5491\n",
      "Epoch: 19 | train_loss: 1.1793 | train_acc: 0.5728 | test_loss: 1.1939 | test_acc: 0.5708\n",
      "Epoch: 20 | train_loss: 1.1613 | train_acc: 0.5803 | test_loss: 1.1890 | test_acc: 0.5626\n",
      "Epoch: 21 | train_loss: 1.1384 | train_acc: 0.5906 | test_loss: 1.1978 | test_acc: 0.5719\n",
      "Epoch: 22 | train_loss: 1.1317 | train_acc: 0.5923 | test_loss: 1.1914 | test_acc: 0.5706\n",
      "Epoch: 23 | train_loss: 1.1212 | train_acc: 0.5952 | test_loss: 1.1638 | test_acc: 0.5746\n",
      "Epoch: 24 | train_loss: 1.1034 | train_acc: 0.6010 | test_loss: 1.1784 | test_acc: 0.5740\n",
      "Epoch: 25 | train_loss: 1.0943 | train_acc: 0.6044 | test_loss: 1.1711 | test_acc: 0.5752\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vit=ViT(patch_size,num_heads)\n",
    "optimizer = optim.Adam(vit.parameters(), lr=5e-3)\n",
    "loss=nn.CrossEntropyLoss()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "results =engine.train( model=vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss,\n",
    "                       epochs=epochs,\n",
    "                       device=device)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('LogitAdjustmentLongTail')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49598912cb7efc65e0007e347a7051cc5ac4c91b95dad2ffbc631da6724968c7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
